{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Author: Ngo Van Uc\n",
    "Date: 23/08/2024\n",
    "Gmail: ngovanuc.1508@gmail.com\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 00. Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>comment</th>\n",
       "      <th>n_star</th>\n",
       "      <th>date_time</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Mới mua máy này Tại thegioididong thốt nốt cảm...</td>\n",
       "      <td>5</td>\n",
       "      <td>2 tuần trước</td>\n",
       "      <td>{CAMERA#Positive};{FEATURES#Positive};{BATTERY...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Pin kém còn lại miễn chê mua 8/3/2019 tình trạ...</td>\n",
       "      <td>5</td>\n",
       "      <td>14/09/2019</td>\n",
       "      <td>{BATTERY#Negative};{GENERAL#Positive};{OTHERS};</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Sao lúc gọi điện thoại màn hình bị chấm nhỏ nh...</td>\n",
       "      <td>3</td>\n",
       "      <td>17/08/2020</td>\n",
       "      <td>{FEATURES#Negative};</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Mọi người cập nhật phần mềm lại , nó sẽ bớt tố...</td>\n",
       "      <td>3</td>\n",
       "      <td>29/02/2020</td>\n",
       "      <td>{FEATURES#Negative};{BATTERY#Neutral};{GENERAL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Mới mua Sài được 1 tháng thấy pin rất trâu, Sà...</td>\n",
       "      <td>5</td>\n",
       "      <td>4/6/2020</td>\n",
       "      <td>{BATTERY#Positive};{PERFORMANCE#Positive};{SER...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                            comment  n_star  \\\n",
       "0      0  Mới mua máy này Tại thegioididong thốt nốt cảm...       5   \n",
       "1      1  Pin kém còn lại miễn chê mua 8/3/2019 tình trạ...       5   \n",
       "2      2  Sao lúc gọi điện thoại màn hình bị chấm nhỏ nh...       3   \n",
       "3      3  Mọi người cập nhật phần mềm lại , nó sẽ bớt tố...       3   \n",
       "4      4  Mới mua Sài được 1 tháng thấy pin rất trâu, Sà...       5   \n",
       "\n",
       "      date_time                                              label  \n",
       "0  2 tuần trước  {CAMERA#Positive};{FEATURES#Positive};{BATTERY...  \n",
       "1    14/09/2019    {BATTERY#Negative};{GENERAL#Positive};{OTHERS};  \n",
       "2    17/08/2020                               {FEATURES#Negative};  \n",
       "3    29/02/2020  {FEATURES#Negative};{BATTERY#Neutral};{GENERAL...  \n",
       "4      4/6/2020  {BATTERY#Positive};{PERFORMANCE#Positive};{SER...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_path = 'E:/IAD/Intern/11_aspectsAnalysis/data/Train.csv'\n",
    "test_path = 'E:/IAD/Intern/11_aspectsAnalysis/data/Test.csv'\n",
    "dev_path = 'E:/IAD/Intern/11_aspectsAnalysis/data/Dev.csv'\n",
    "\n",
    "train_data = pd.read_csv(train_path)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_to_id = {\"SCREEN\": 0, \"CAMERA\": 1, \"FEATURES\": 2, \"BATTERY\": 3, \"PERFORMANCE\": 4, \"STORAGE\": 5, \"DESIGN\": 6, \"PRICE\": 7, \"GENERAL\": 8, \"SER&ACC\": 9} \n",
    "label_to_id = {\"Positive\": 0, \"Negative\": 1, \"Neutral\": 2, \"None\": 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_dict(sentences_label):\n",
    "    label_dict = {}\n",
    "    labels = sentences_label.split(';')[:-1]\n",
    "    if len(labels) == 1 and labels[-1][1:-1] == \"OTHERS\":\n",
    "        return None\n",
    "    else:\n",
    "        if labels[-1][1:-1] == \"OTHERS\":\n",
    "            for l in labels[:-1]:\n",
    "                class_name, sentiment = l[1:-1].split('#')\n",
    "                label_dict[class_name] = sentiment\n",
    "            return label_dict\n",
    "        else:\n",
    "            for l in labels:\n",
    "                class_name, sentiment = l[1:-1].split('#')\n",
    "                label_dict[class_name] = sentiment\n",
    "            return label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mới mua máy này Tại thegioididong thốt nốt cảm thấy ok bin trâu chụp ảnh đẹp loa nghe to bắt wf khỏe sóng ổn định, giá thành vừa với túi tiền, nhân viên tư vấn nhiệt tình\n",
      "[[0, 0, 0, 1], [1, 0, 0, 0], [1, 0, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 0, 0, 0], [1, 0, 0, 0], [1, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "sentences = train_data['comment'].tolist()\n",
    "label_comment = train_data['label'].tolist()\n",
    "others_label = [[0, 0, 0, 1]] * 10\n",
    "        \n",
    "train_set = []\n",
    "\n",
    "for sentence, sentence_label in zip(sentences, label_comment):\n",
    "    label_dict = get_label_dict(sentence_label)\n",
    "    labels_ = []\n",
    "    if label_dict:\n",
    "        for aspect in list(class_to_id.keys()):\n",
    "            train_label = [0] * 4\n",
    "            if aspect in label_dict.keys():\n",
    "                label_id = label_to_id[label_dict[aspect]]\n",
    "                train_label[label_id] =  1\n",
    "                labels_.append(train_label)\n",
    "            else:\n",
    "                labels_.append([0, 0, 0, 1])\n",
    "        train_set.append(\n",
    "            {'sentence': sentence, 'labels': labels_}\n",
    "        )\n",
    "    else:\n",
    "        train_set.append(\n",
    "            {'sentence': sentence, 'labels': others_label}\n",
    "        )\n",
    "print(train_set[0]['sentence'])\n",
    "print(train_set[0]['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pass\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(train_set):\n",
    "    sentence, label = data.items()\n",
    "    input = np.array(label[-1])\n",
    "    clear_output()\n",
    "    if input.shape == (10, 4):\n",
    "        print(\"pass\")\n",
    "    else:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "\n",
    "class AspectSentimentDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sentence = self.data[index]['sentence']\n",
    "        labels = self.data[index]['labels']\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            sentence,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'sentence_text': sentence,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(labels, dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('google-bert/bert-base-multilingual-cased')\n",
    "dataset = AspectSentimentDataset(train_set, tokenizer, max_len=128)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7786"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "\n",
    "\n",
    "class AspectSentimentModel(nn.Module):\n",
    "    def __init__(self, n_classes, n_labels_per_class):\n",
    "        super(AspectSentimentModel, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('google-bert/bert-base-multilingual-cased')\n",
    "        self.lstm = nn.LSTM(768, 256, batch_first=True, bidirectional=True)\n",
    "        self.classifier = nn.Linear(256*2, n_classes * n_labels_per_class)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        bert_outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        lstm_outputs, _ = self.lstm(bert_outputs.last_hidden_state)\n",
    "        lstm_outputs = lstm_outputs[:, -1, :]\n",
    "        output = self.classifier(lstm_outputs)\n",
    "        return output.view(-1, 10, 4)\n",
    "    \n",
    "model = AspectSentimentModel(n_classes=10, n_labels_per_class=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04. Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def train_epoch(model, data_loader, loss_fn, optimizer, device):\n",
    "    model = model.train()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for data in data_loader:\n",
    "        input_ids = data['input_ids'].to(device)\n",
    "        attention_mask = data['attention_mask'].to(device)\n",
    "        labels = data['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return np.mean(losses)\n",
    "\n",
    "\n",
    "EPOCHS = 10\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "training = True\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "if training == True:\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss = train_epoch(model, dataloader, loss_fn, optimizer, device)\n",
    "        print(f'Epoch {epoch + 1}/{EPOCHS}, Train loss: {train_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_save_model = 'E:/IAD/Intern/11_aspectsAnalysis/models'\n",
    "torch.save(model.state_dict(), path_save_model+'/model_v2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = AspectSentimentModel(n_classes=10, n_labels_per_class=4)\n",
    "# model.load_state_dict(torch.load(path_save_model+'/model_v2.pth'))\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(model, sentence, tokenizer, max_len, device):\n",
    "    model = model.eval()\n",
    "\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        sentence,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_len,\n",
    "        return_token_type_ids=False,\n",
    "        pad_to_max_length=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def interpret_predictions(predictions):\n",
    "    aspect_classes = ['SCREEN', 'CAMERA', 'FEATURES', 'BATTERY', 'PERFORMANCE', 'STORAGE', 'DESIGN', 'PRICE', 'GENERAL', 'SER&ACC']\n",
    "    sentiment_labels = ['Positive', 'Negative', 'Neutral', 'None']\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for i, aspect in enumerate(aspect_classes):\n",
    "        aspect_sentiments = predictions[0, i].cpu().numpy()\n",
    "        sentiment_index = aspect_sentiments.argmax()\n",
    "        results[aspect] = sentiment_labels[sentiment_index]\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05. Thực hiện predict trên bộ dữ liệu kiểm thử - test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>comment</th>\n",
       "      <th>n_star</th>\n",
       "      <th>date_time</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Điện thoải ổn. Facelock cực nhanh, vân tay ôk ...</td>\n",
       "      <td>5</td>\n",
       "      <td>5/2/2020</td>\n",
       "      <td>{SCREEN#Positive};{FEATURES#Positive};{PERFORM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Mình mới  mua vivo91c. Tải ứng dụng ,games  nh...</td>\n",
       "      <td>5</td>\n",
       "      <td>14/05/2019</td>\n",
       "      <td>{FEATURES#Negative};{PERFORMANCE#Positive};{SE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Xấu đẹp gì ko biết nhưng rất ưng TGdđ phục vụ ...</td>\n",
       "      <td>5</td>\n",
       "      <td>26/03/2020</td>\n",
       "      <td>{DESIGN#Neutral};{SER&amp;ACC#Positive};</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Màn hình hơi lác khi chơi game. Game nặng thì ...</td>\n",
       "      <td>4</td>\n",
       "      <td>4/6/2019</td>\n",
       "      <td>{PERFORMANCE#Negative};{DESIGN#Negative};{OTHE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Nói chung máy đẹp với màn amoled, ổn trong tầm...</td>\n",
       "      <td>4</td>\n",
       "      <td>12/5/2020</td>\n",
       "      <td>{SCREEN#Positive};{BATTERY#Negative};{DESIGN#P...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                            comment  n_star  \\\n",
       "0      0  Điện thoải ổn. Facelock cực nhanh, vân tay ôk ...       5   \n",
       "1      1  Mình mới  mua vivo91c. Tải ứng dụng ,games  nh...       5   \n",
       "2      2  Xấu đẹp gì ko biết nhưng rất ưng TGdđ phục vụ ...       5   \n",
       "3      3  Màn hình hơi lác khi chơi game. Game nặng thì ...       4   \n",
       "4      4  Nói chung máy đẹp với màn amoled, ổn trong tầm...       4   \n",
       "\n",
       "    date_time                                              label  \n",
       "0    5/2/2020  {SCREEN#Positive};{FEATURES#Positive};{PERFORM...  \n",
       "1  14/05/2019  {FEATURES#Negative};{PERFORMANCE#Positive};{SE...  \n",
       "2  26/03/2020               {DESIGN#Neutral};{SER&ACC#Positive};  \n",
       "3    4/6/2019  {PERFORMANCE#Negative};{DESIGN#Negative};{OTHE...  \n",
       "4   12/5/2020  {SCREEN#Positive};{BATTERY#Negative};{DESIGN#P...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv(test_path)\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mọi thứ ok.mình thấy pin trâu chứ có sao đâu.mua đc 2 tuần rồi chỉ có cảm biến vân tay ko nhạy như mong đợi thôi.\n",
      "{FEATURES#Negative};{BATTERY#Positive};{GENERAL#Positive};\n",
      "Aspect Sentiment Prediction:\n",
      "SCREEN: None\n",
      "CAMERA: None\n",
      "FEATURES: Negative\n",
      "BATTERY: Positive\n",
      "PERFORMANCE: None\n",
      "STORAGE: None\n",
      "DESIGN: None\n",
      "PRICE: None\n",
      "GENERAL: Positive\n",
      "SER&ACC: None\n"
     ]
    }
   ],
   "source": [
    "max_len = 256\n",
    "idx = 20\n",
    "sentence_test = test_data.loc[idx, 'comment']\n",
    "true_label = test_data.loc[idx, 'label']\n",
    "print(sentence_test)\n",
    "print(true_label)\n",
    "\n",
    "predictions = predict_sentiment(model, sentence_test, tokenizer, max_len, device)\n",
    "results = interpret_predictions(predictions)\n",
    "\n",
    "print('Aspect Sentiment Prediction:')\n",
    "for aspect, sentiment in results.items():\n",
    "    print(f'{aspect}: {sentiment}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done!\n"
     ]
    }
   ],
   "source": [
    "predictions_on_test = []\n",
    "\n",
    "for i in range(len(test_data)):\n",
    "    print(i)\n",
    "    sentence_on_test = test_data['comment'][i]\n",
    "    prediction_on_test = predict_sentiment(model, sentence_on_test, tokenizer, max_len, device)\n",
    "    results = interpret_predictions(prediction_on_test)\n",
    "    predictions_on_test.append(results)\n",
    "    clear_output()\n",
    "\n",
    "print('All done!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = []\n",
    "\n",
    "for i in range(len(test_data)):\n",
    "    outcome = {}\n",
    "    label = test_data['label'][i]\n",
    "    label = label.replace('{', '')\n",
    "    label = label.replace('}', '')\n",
    "    labels = label.split(';')\n",
    "    for l in labels:\n",
    "        try:\n",
    "            text = l.split('#')\n",
    "            outcome[text[0]] = text[1]\n",
    "        except:\n",
    "            pass\n",
    "    true_labels.append(outcome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06. Chuẩn hóa nhãn đầu ra để thực hiện đánh giá hiệu suất mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_label(true_label, prediction):\n",
    "    '''true_label: dict\n",
    "     prediction: dict '''\n",
    "    for key, value in prediction.items():\n",
    "        if key not in true_label:\n",
    "            true_label[key] = 'None'\n",
    "    return true_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mục tiêu là đưa danh sách các nhãn thực tế về dạng đánh giá 10 class như trong dự đoán\n",
    "true_full_labels = [] \n",
    "for idx in range(len(predictions_on_test)):\n",
    "    true_label = true_labels[idx]\n",
    "    prediction = predictions_on_test[idx]\n",
    "    true_full_labels.append(full_label(true_label, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SCREEN': 'None', 'CAMERA': 'None', 'FEATURES': 'None', 'BATTERY': 'None', 'PERFORMANCE': 'None', 'STORAGE': 'None', 'DESIGN': 'None', 'PRICE': 'None', 'GENERAL': 'Positive', 'SER&ACC': 'Positive'}\n",
      "{'SCREEN': 'None', 'CAMERA': 'None', 'FEATURES': 'None', 'BATTERY': 'None', 'PERFORMANCE': 'None', 'STORAGE': 'None', 'DESIGN': 'None', 'PRICE': 'None', 'GENERAL': 'None', 'SER&ACC': 'None'}\n",
      "Thật tuyệt máy qua mượt túi thích  như thế giới Di Động có chỗ giá khác túi không hiểu đánh giá 5sao cho thế giới Di Động phương Phú Châu 5sao nhân viên OK\n",
      "true:  SCREEN None\n",
      "predicted:  SCREEN None\n",
      "true:  CAMERA None\n",
      "predicted:  CAMERA None\n",
      "true:  FEATURES None\n",
      "predicted:  FEATURES None\n",
      "true:  BATTERY None\n",
      "predicted:  BATTERY None\n",
      "true:  PERFORMANCE None\n",
      "predicted:  PERFORMANCE None\n",
      "true:  STORAGE None\n",
      "predicted:  STORAGE None\n",
      "true:  DESIGN None\n",
      "predicted:  DESIGN None\n",
      "true:  PRICE None\n",
      "predicted:  PRICE None\n",
      "true:  GENERAL None\n",
      "predicted:  GENERAL Positive\n",
      "true:  SER&ACC None\n",
      "predicted:  SER&ACC Positive\n"
     ]
    }
   ],
   "source": [
    "# data thỉnh thoảng bị sai -..-\n",
    "# bằng chứng là SER&ACC trong câu này phải là Positive nhưng nhãn là None\n",
    "# hoặc GENERAL nên là Positive thì nhãn lại là None\n",
    "\n",
    "print(predictions_on_test[6])\n",
    "print(true_labels[6])\n",
    "print(test_data.loc[6, 'comment'])\n",
    "for key, value in true_labels[6].items():\n",
    "    print('true: ',key, value)\n",
    "    print('predicted: ', key, predictions_on_test[6][key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07. Đánh giá hiệu suất tổng thể mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.9704226066663284\n",
      "Precision = 0.8779313232830821\n",
      "Recall = 1.0\n",
      "f1 = 0.9349983275727506\n"
     ]
    }
   ],
   "source": [
    "true_ = []\n",
    "predicted_ = []\n",
    "\n",
    "for idx in range(len(predictions_on_test)):\n",
    "    for key, value in predictions_on_test[idx].items():\n",
    "        if true_full_labels[idx][key] == 'None' and value == 'None':\n",
    "            true_.append(0)\n",
    "            predicted_.append(0)\n",
    "        elif true_full_labels[idx][key] != 'None' and true_full_labels[idx][key] == value:\n",
    "            true_.append(1)\n",
    "            predicted_.append(1)\n",
    "        elif true_full_labels[idx][key] == 'None' and value != 'None':\n",
    "            true_.append(0)\n",
    "            predicted_.append(1)\n",
    "        elif true_full_labels[idx][key] != 'None' and value == 'None':\n",
    "            true_.append(1)\n",
    "            predicted_.append(0)\n",
    "\n",
    "accuracy = accuracy_score(true_, predicted_)\n",
    "precision = precision_score(true_, predicted_)\n",
    "recall = recall_score(true_, predicted_)\n",
    "f1 = f1_score(true_, predicted_)\n",
    "\n",
    "print(f'Accuracy = {accuracy}')\n",
    "print(f'Precision = {precision}')\n",
    "print(f'Recall = {recall}')\n",
    "print(f'f1 = {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 08. Đánh giá hiệu suất nhận diện khía cạnh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy, Precision, Recall, F1-score trên toàn bộ khía cạnh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 0.8919169447534298\n",
      "Recall = 0.7157096102350491\n",
      "F1-score = 0.7941564872895345\n"
     ]
    }
   ],
   "source": [
    "true_ = []\n",
    "predicted_ = []\n",
    "\n",
    "for idx in range(len(predictions_on_test)):\n",
    "    for key, value in predictions_on_test[idx].items():\n",
    "        if true_full_labels[idx] == 'None' and value == 'None':\n",
    "            true_.append(0)\n",
    "            predicted_.append(0)\n",
    "        elif true_full_labels[idx][key] != 'None' and value != 'None':\n",
    "            true_.append(1)\n",
    "            predicted_.append(1)\n",
    "        elif true_full_labels[idx][key] == 'None' and value != 'None':\n",
    "            true_.append(0)\n",
    "            predicted_.append(1)\n",
    "        elif true_full_labels[idx][key] != 'None' and value == 'None':\n",
    "            true_.append(1)\n",
    "            predicted_.append(0)\n",
    "\n",
    "print(f'Accuracy = {accuracy_score(true_, predicted_)}')\n",
    "print(f'Precision = {precision_score(true_, predicted_)}')\n",
    "print(f'Recall = {recall_score(true_, predicted_)}')\n",
    "print(f'F1-score = {f1_score(true_, predicted_)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### đánh giá hiệu suất khía cạnh trên class SCREEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.9523381294964028\n",
      "Precision = 0.8558951965065502\n",
      "Recall = 0.7286245353159851\n",
      "f1 = 0.7871485943775101\n"
     ]
    }
   ],
   "source": [
    "true_ = []\n",
    "predicted_ = []\n",
    "\n",
    "for idx in range(len(true_full_labels)):\n",
    "    key = 'SCREEN'\n",
    "    if true_full_labels[idx][key] == 'None' and predictions_on_test[idx][key] == 'None':\n",
    "        true_.append(0)\n",
    "        predicted_.append(0)\n",
    "    elif true_full_labels[idx][key] != 'None' and predictions_on_test[idx][key] != 'None':\n",
    "        true_.append(1)\n",
    "        predicted_.append(1)\n",
    "    elif true_full_labels[idx][key] == 'None' and predictions_on_test[idx][key] != 'None':\n",
    "        true_.append(0)\n",
    "        predicted_.append(1)\n",
    "    elif true_full_labels[idx][key] != 'None' and predictions_on_test[idx][key] == 'None':\n",
    "        true_.append(1)\n",
    "        predicted_.append(0)\n",
    "        \n",
    "accuracy = accuracy_score(true_, predicted_)\n",
    "precision = precision_score(true_, predicted_)\n",
    "recall = recall_score(true_, predicted_)\n",
    "f1 = f1_score(true_, predicted_)\n",
    "\n",
    "print(f'Accuracy = {accuracy}')\n",
    "print(f'Precision = {precision}')\n",
    "print(f'Recall = {recall}')\n",
    "print(f'f1 = {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### đánh giá hiệu suất khía cạnh trên class FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.9159172661870504\n",
      "Precision = 0.861878453038674\n",
      "Recall = 0.8776371308016878\n",
      "f1 = 0.8696864111498258\n"
     ]
    }
   ],
   "source": [
    "true_ = []\n",
    "predicted_ = []\n",
    "\n",
    "for idx in range(len(true_full_labels)):\n",
    "    key = 'FEATURES'\n",
    "    if true_full_labels[idx][key] == 'None' and predictions_on_test[idx][key] == 'None':\n",
    "        true_.append(0)\n",
    "        predicted_.append(0)\n",
    "    elif true_full_labels[idx][key] != 'None' and predictions_on_test[idx][key] != 'None':\n",
    "        true_.append(1)\n",
    "        predicted_.append(1)\n",
    "    elif true_full_labels[idx][key] == 'None' and predictions_on_test[idx][key] != 'None':\n",
    "        true_.append(0)\n",
    "        predicted_.append(1)\n",
    "    elif true_full_labels[idx][key] != 'None' and predictions_on_test[idx][key] == 'None':\n",
    "        true_.append(1)\n",
    "        predicted_.append(0)\n",
    "        \n",
    "accuracy = accuracy_score(true_, predicted_)\n",
    "precision = precision_score(true_, predicted_)\n",
    "recall = recall_score(true_, predicted_)\n",
    "f1 = f1_score(true_, predicted_)\n",
    "\n",
    "print(f'Accuracy = {accuracy}')\n",
    "print(f'Precision = {precision}')\n",
    "print(f'Recall = {recall}')\n",
    "print(f'f1 = {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### đánh giá hiệu suất khía cạnh trên class PERFORMANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.47302158273381295\n",
      "Precision = 0.0\n",
      "Recall = 0.0\n",
      "f1 = 0.0\n"
     ]
    }
   ],
   "source": [
    "true_ = []\n",
    "predicted_ = []\n",
    "\n",
    "for idx in range(len(true_full_labels)):\n",
    "    key = 'PERFORMANCE'\n",
    "    if true_full_labels[idx][key] == 'None' and predictions_on_test[idx][key] == 'None':\n",
    "        true_.append(0)\n",
    "        predicted_.append(0)\n",
    "    elif true_full_labels[idx][key] != 'None' and predictions_on_test[idx][key] != 'None':\n",
    "        true_.append(1)\n",
    "        predicted_.append(1)\n",
    "    elif true_full_labels[idx][key] == 'None' and predictions_on_test[idx][key] != 'None':\n",
    "        true_.append(0)\n",
    "        predicted_.append(1)\n",
    "    elif true_full_labels[idx][key] != 'None' and predictions_on_test[idx][key] == 'None':\n",
    "        true_.append(1)\n",
    "        predicted_.append(0)\n",
    "        \n",
    "accuracy = accuracy_score(true_, predicted_)\n",
    "precision = precision_score(true_, predicted_)\n",
    "recall = recall_score(true_, predicted_)\n",
    "f1 = f1_score(true_, predicted_)\n",
    "\n",
    "print(f'Accuracy = {accuracy}')\n",
    "print(f'Precision = {precision}')\n",
    "print(f'Recall = {recall}')\n",
    "print(f'f1 = {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### đánh giá hiệu suất khía cạnh trên class GENERAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.91681654676259\n",
      "Precision = 1.0\n",
      "Recall = 0.8818646232439336\n",
      "f1 = 0.9372242958941296\n"
     ]
    }
   ],
   "source": [
    "true_ = []\n",
    "predicted_ = []\n",
    "\n",
    "for idx in range(len(true_full_labels)):\n",
    "    key = 'GENERAL'\n",
    "    if true_full_labels[idx][key] == 'None' and predictions_on_test[idx][key] == 'None':\n",
    "        true_.append(0)\n",
    "        predicted_.append(0)\n",
    "    elif true_full_labels[idx][key] != 'None' and predictions_on_test[idx][key] != 'None':\n",
    "        true_.append(1)\n",
    "        predicted_.append(1)\n",
    "    elif true_full_labels[idx][key] == 'None' and predictions_on_test[idx][key] != 'None':\n",
    "        true_.append(0)\n",
    "        predicted_.append(1)\n",
    "    elif true_full_labels[idx][key] != 'None' and predictions_on_test[idx][key] == 'None':\n",
    "        true_.append(1)\n",
    "        predicted_.append(0)\n",
    "        \n",
    "accuracy = accuracy_score(true_, predicted_)\n",
    "precision = precision_score(true_, predicted_)\n",
    "recall = recall_score(true_, predicted_)\n",
    "f1 = f1_score(true_, predicted_)\n",
    "\n",
    "print(f'Accuracy = {accuracy}')\n",
    "print(f'Precision = {precision}')\n",
    "print(f'Recall = {recall}')\n",
    "print(f'f1 = {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### đánh giá hiệu suất khía cạnh trên class BATTERY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.9941546762589928\n",
      "Precision = 1.0\n",
      "Recall = 0.9878163074039362\n",
      "f1 = 0.9938708156529938\n"
     ]
    }
   ],
   "source": [
    "true_ = []\n",
    "predicted_ = []\n",
    "\n",
    "for idx in range(len(true_full_labels)):\n",
    "    key = 'BATTERY'\n",
    "    if true_full_labels[idx][key] == 'None' and predictions_on_test[idx][key] == 'None':\n",
    "        true_.append(0)\n",
    "        predicted_.append(0)\n",
    "    elif true_full_labels[idx][key] != 'None' and predictions_on_test[idx][key] != 'None':\n",
    "        true_.append(1)\n",
    "        predicted_.append(1)\n",
    "    elif true_full_labels[idx][key] == 'None' and predictions_on_test[idx][key] != 'None':\n",
    "        true_.append(0)\n",
    "        predicted_.append(1)\n",
    "    elif true_full_labels[idx][key] != 'None' and predictions_on_test[idx][key] == 'None':\n",
    "        true_.append(1)\n",
    "        predicted_.append(0)\n",
    "        \n",
    "accuracy = accuracy_score(true_, predicted_)\n",
    "precision = precision_score(true_, predicted_)\n",
    "recall = recall_score(true_, predicted_)\n",
    "f1 = f1_score(true_, predicted_)\n",
    "\n",
    "print(f'Accuracy = {accuracy}')\n",
    "print(f'Precision = {precision}')\n",
    "print(f'Recall = {recall}')\n",
    "print(f'f1 = {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### đánh giá hiệu suất khía cạnh trên class STORAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.9878597122302158\n",
      "Precision = 0.0\n",
      "Recall = 0.0\n",
      "f1 = 0.0\n"
     ]
    }
   ],
   "source": [
    "true_ = []\n",
    "predicted_ = []\n",
    "\n",
    "for idx in range(len(true_full_labels)):\n",
    "    key = 'STORAGE'\n",
    "    if true_full_labels[idx][key] == 'None' and predictions_on_test[idx][key] == 'None':\n",
    "        true_.append(0)\n",
    "        predicted_.append(0)\n",
    "    elif true_full_labels[idx][key] != 'None' and predictions_on_test[idx][key] != 'None':\n",
    "        true_.append(1)\n",
    "        predicted_.append(1)\n",
    "    elif true_full_labels[idx][key] == 'None' and predictions_on_test[idx][key] != 'None':\n",
    "        true_.append(0)\n",
    "        predicted_.append(1)\n",
    "    elif true_full_labels[idx][key] != 'None' and predictions_on_test[idx][key] == 'None':\n",
    "        true_.append(1)\n",
    "        predicted_.append(0)\n",
    "        \n",
    "accuracy = accuracy_score(true_, predicted_)\n",
    "precision = precision_score(true_, predicted_)\n",
    "recall = recall_score(true_, predicted_)\n",
    "f1 = f1_score(true_, predicted_)\n",
    "\n",
    "print(f'Accuracy = {accuracy}')\n",
    "print(f'Precision = {precision}')\n",
    "print(f'Recall = {recall}')\n",
    "print(f'f1 = {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### đánh giá hiệu suất khía cạnh trên class DESIGN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.9545863309352518\n",
      "Precision = 1.0\n",
      "Recall = 0.7651162790697674\n",
      "f1 = 0.8669301712779973\n"
     ]
    }
   ],
   "source": [
    "true_ = []\n",
    "predicted_ = []\n",
    "\n",
    "for idx in range(len(true_full_labels)):\n",
    "    key = 'DESIGN'\n",
    "    if true_full_labels[idx][key] == 'None' and predictions_on_test[idx][key] == 'None':\n",
    "        true_.append(0)\n",
    "        predicted_.append(0)\n",
    "    elif true_full_labels[idx][key] != 'None' and predictions_on_test[idx][key] != 'None':\n",
    "        true_.append(1)\n",
    "        predicted_.append(1)\n",
    "    elif true_full_labels[idx][key] == 'None' and predictions_on_test[idx][key] != 'None':\n",
    "        true_.append(0)\n",
    "        predicted_.append(1)\n",
    "    elif true_full_labels[idx][key] != 'None' and predictions_on_test[idx][key] == 'None':\n",
    "        true_.append(1)\n",
    "        predicted_.append(0)\n",
    "        \n",
    "accuracy = accuracy_score(true_, predicted_)\n",
    "precision = precision_score(true_, predicted_)\n",
    "recall = recall_score(true_, predicted_)\n",
    "f1 = f1_score(true_, predicted_)\n",
    "\n",
    "print(f'Accuracy = {accuracy}')\n",
    "print(f'Precision = {precision}')\n",
    "print(f'Recall = {recall}')\n",
    "print(f'f1 = {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### đánh giá hiệu suất khía cạnh trên class PRICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.9676258992805755\n",
      "Precision = 1.0\n",
      "Recall = 0.8862559241706162\n",
      "f1 = 0.9396984924623115\n"
     ]
    }
   ],
   "source": [
    "true_ = []\n",
    "predicted_ = []\n",
    "\n",
    "for idx in range(len(true_full_labels)):\n",
    "    key = 'PRICE'\n",
    "    if true_full_labels[idx][key] == 'None' and predictions_on_test[idx][key] == 'None':\n",
    "        true_.append(0)\n",
    "        predicted_.append(0)\n",
    "    elif true_full_labels[idx][key] != 'None' and predictions_on_test[idx][key] != 'None':\n",
    "        true_.append(1)\n",
    "        predicted_.append(1)\n",
    "    elif true_full_labels[idx][key] == 'None' and predictions_on_test[idx][key] != 'None':\n",
    "        true_.append(0)\n",
    "        predicted_.append(1)\n",
    "    elif true_full_labels[idx][key] != 'None' and predictions_on_test[idx][key] == 'None':\n",
    "        true_.append(1)\n",
    "        predicted_.append(0)\n",
    "        \n",
    "accuracy = accuracy_score(true_, predicted_)\n",
    "precision = precision_score(true_, predicted_)\n",
    "recall = recall_score(true_, predicted_)\n",
    "f1 = f1_score(true_, predicted_)\n",
    "\n",
    "print(f'Accuracy = {accuracy}')\n",
    "print(f'Precision = {precision}')\n",
    "print(f'Recall = {recall}')\n",
    "print(f'f1 = {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### đánh giá hiệu suất khía cạnh trên class SER&ACC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.9312050359712231\n",
      "Precision = 1.0\n",
      "Recall = 0.7695783132530121\n",
      "f1 = 0.8697872340425532\n"
     ]
    }
   ],
   "source": [
    "true_ = []\n",
    "predicted_ = []\n",
    "\n",
    "for idx in range(len(true_full_labels)):\n",
    "    key = 'SER&ACC'\n",
    "    if true_full_labels[idx][key] == 'None' and predictions_on_test[idx][key] == 'None':\n",
    "        true_.append(0)\n",
    "        predicted_.append(0)\n",
    "    elif true_full_labels[idx][key] != 'None' and predictions_on_test[idx][key] != 'None':\n",
    "        true_.append(1)\n",
    "        predicted_.append(1)\n",
    "    elif true_full_labels[idx][key] == 'None' and predictions_on_test[idx][key] != 'None':\n",
    "        true_.append(0)\n",
    "        predicted_.append(1)\n",
    "    elif true_full_labels[idx][key] != 'None' and predictions_on_test[idx][key] == 'None':\n",
    "        true_.append(1)\n",
    "        predicted_.append(0)\n",
    "        \n",
    "accuracy = accuracy_score(true_, predicted_)\n",
    "precision = precision_score(true_, predicted_)\n",
    "recall = recall_score(true_, predicted_)\n",
    "f1 = f1_score(true_, predicted_)\n",
    "\n",
    "print(f'Accuracy = {accuracy}')\n",
    "print(f'Precision = {precision}')\n",
    "print(f'Recall = {recall}')\n",
    "print(f'f1 = {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### đánh giá hiệu suất khía cạnh trên class CAMERA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.987410071942446\n",
      "Precision = 1.0\n",
      "Recall = 0.9557661927330173\n",
      "f1 = 0.9773828756058158\n"
     ]
    }
   ],
   "source": [
    "true_ = []\n",
    "predicted_ = []\n",
    "\n",
    "for idx in range(len(true_full_labels)):\n",
    "    key = 'CAMERA'\n",
    "    if true_full_labels[idx][key] == 'None' and predictions_on_test[idx][key] == 'None':\n",
    "        true_.append(0)\n",
    "        predicted_.append(0)\n",
    "    elif true_full_labels[idx][key] != 'None' and predictions_on_test[idx][key] != 'None':\n",
    "        true_.append(1)\n",
    "        predicted_.append(1)\n",
    "    elif true_full_labels[idx][key] == 'None' and predictions_on_test[idx][key] != 'None':\n",
    "        true_.append(0)\n",
    "        predicted_.append(1)\n",
    "    elif true_full_labels[idx][key] != 'None' and predictions_on_test[idx][key] == 'None':\n",
    "        true_.append(1)\n",
    "        predicted_.append(0)\n",
    "        \n",
    "accuracy = accuracy_score(true_, predicted_)\n",
    "precision = precision_score(true_, predicted_)\n",
    "recall = recall_score(true_, predicted_)\n",
    "f1 = f1_score(true_, predicted_)\n",
    "\n",
    "print(f'Accuracy = {accuracy}')\n",
    "print(f'Precision = {precision}')\n",
    "print(f'Recall = {recall}')\n",
    "print(f'f1 = {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 09. Đánh giá hiệu suất nhận diện tình cảm (đa nhãn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy trên toàn bộ data kiểm thử\n",
    "acc = corrects/(all predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.6537987679671458\n"
     ]
    }
   ],
   "source": [
    "true_ = []\n",
    "predicted_ = []\n",
    "accuracy = []\n",
    "\n",
    "for idx in range(len(true_full_labels)):\n",
    "    for key, value in true_full_labels[idx].items():\n",
    "        if value == 'None':\n",
    "            continue\n",
    "        elif predictions_on_test[idx][key] != value:\n",
    "            accuracy.append(0)\n",
    "        elif predictions_on_test[idx][key] == value:\n",
    "            accuracy.append(1)\n",
    "        \n",
    "print(f'Accuracy = {sum(accuracy)/len(accuracy)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precisio, Recall, F1-score tình cảm trên toàn bộ data kiểm thử"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report:\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "    BATTERY_negative     0.8523    0.8866    0.8691       397\n",
      "     BATTERY_neutral     0.0000    0.0000    0.0000        92\n",
      "        BATTERY_none     0.9889    1.0000    0.9944      1157\n",
      "    BATTERY_positive     0.8549    0.9481    0.8991       578\n",
      "     CAMERA_negative     0.8173    0.8173    0.8173       197\n",
      "      CAMERA_neutral     0.5000    0.0141    0.0274        71\n",
      "         CAMERA_none     0.9827    1.0000    0.9913      1591\n",
      "     CAMERA_positive     0.8522    0.9479    0.8975       365\n",
      "     DESIGN_negative     0.8929    0.2381    0.3759       105\n",
      "      DESIGN_neutral     0.0000    0.0000    0.0000        28\n",
      "         DESIGN_none     0.9467    1.0000    0.9726      1794\n",
      "     DESIGN_positive     0.8738    0.8855    0.8796       297\n",
      "   FEATURES_negative     0.9051    0.8517    0.8776       526\n",
      "    FEATURES_neutral     0.0000    0.0000    0.0000        52\n",
      "       FEATURES_none     0.9420    1.0000    0.9701      1413\n",
      "   FEATURES_positive     0.8166    0.8026    0.8095       233\n",
      "    GENERAL_negative     0.9355    0.7230    0.8156       361\n",
      "     GENERAL_neutral     0.0000    0.0000    0.0000        83\n",
      "        GENERAL_none     0.7805    1.0000    0.8767       658\n",
      "    GENERAL_positive     0.9474    0.9305    0.9388      1122\n",
      "PERFORMANCE_negative     0.0000    0.0000    0.0000       454\n",
      " PERFORMANCE_neutral     0.0000    0.0000    0.0000       116\n",
      "    PERFORMANCE_none     0.4730    1.0000    0.6422      1052\n",
      "PERFORMANCE_positive     0.0000    0.0000    0.0000       602\n",
      "      PRICE_negative     0.7500    0.5000    0.6000        90\n",
      "       PRICE_neutral     0.8824    0.7479    0.8096       361\n",
      "          PRICE_none     0.9567    1.0000    0.9779      1591\n",
      "      PRICE_positive     0.7538    0.8077    0.7798       182\n",
      "     SCREEN_negative     0.9167    0.5789    0.7097       133\n",
      "      SCREEN_neutral     0.0000    0.0000    0.0000        17\n",
      "         SCREEN_none     0.9634    1.0000    0.9814      1922\n",
      "     SCREEN_positive     0.8621    0.8224    0.8418       152\n",
      "    SER&ACC_negative     0.8899    0.4826    0.6258       201\n",
      "     SER&ACC_neutral     0.0000    0.0000    0.0000        27\n",
      "        SER&ACC_none     0.9107    1.0000    0.9533      1560\n",
      "    SER&ACC_positive     0.9428    0.8693    0.9045       436\n",
      "    STORAGE_negative     0.0000    0.0000    0.0000         6\n",
      "     STORAGE_neutral     0.0000    0.0000    0.0000         3\n",
      "        STORAGE_none     0.9879    1.0000    0.9939      2197\n",
      "    STORAGE_positive     0.0000    0.0000    0.0000        18\n",
      "\n",
      "            accuracy                         0.8863     22240\n",
      "           macro avg     0.6044    0.5714    0.5708     22240\n",
      "        weighted avg     0.8471    0.8863    0.8585     22240\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for idx in range(len(true_labels)):\n",
    "    for key, value in true_labels[idx].items():\n",
    "        y_true.append(key + \"_\" + value)\n",
    "        y_pred.append(key + \"_\" + predictions_on_test[idx][key])\n",
    "\n",
    "\n",
    "print(f'Classification report:')\n",
    "print(classification_report(y_true, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Precision, Recall, F1-Score:\n",
      "Micro Precision: 0.9550\n",
      "Micro Recall: 0.9550\n",
      "Micro F1-Score: 0.9550\n",
      "Macro Precision: 0.6855\n",
      "Macro Recall: 0.6003\n",
      "Macro F1-Score: 0.6332\n",
      "Weighted Precision: 0.9463\n",
      "Weighted Recall: 0.9550\n",
      "Weighted F1-Score: 0.9481\n"
     ]
    }
   ],
   "source": [
    "precision_micro = precision_score(y_true, y_pred, average='micro')\n",
    "recall_micro = recall_score(y_true, y_pred, average='micro')\n",
    "f1_micro = f1_score(y_true, y_pred, average='micro')\n",
    "\n",
    "precision_macro = precision_score(y_true, y_pred, average='macro')\n",
    "recall_macro = recall_score(y_true, y_pred, average='macro')\n",
    "f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "precision_weighted = precision_score(y_true, y_pred, average='weighted')\n",
    "recall_weighted = recall_score(y_true, y_pred, average='weighted')\n",
    "f1_weighted = f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "print(\"\\nOverall Precision, Recall, F1-Score:\")\n",
    "print(f\"Micro Precision: {precision_micro:.4f}\")\n",
    "print(f\"Micro Recall: {recall_micro:.4f}\")\n",
    "print(f\"Micro F1-Score: {f1_micro:.4f}\")\n",
    "print(f\"Macro Precision: {precision_macro:.4f}\")\n",
    "print(f\"Macro Recall: {recall_macro:.4f}\")\n",
    "print(f\"Macro F1-Score: {f1_macro:.4f}\")\n",
    "print(f\"Weighted Precision: {precision_weighted:.4f}\")\n",
    "print(f\"Weighted Recall: {recall_weighted:.4f}\")\n",
    "print(f\"Weighted F1-Score: {f1_weighted:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Đánh giá hiệu suất nhận diện tình cảm của khía cạnh SCREEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.58      0.71       133\n",
      "     neutral       0.00      0.00      0.00        17\n",
      "        none       0.96      1.00      0.98      1922\n",
      "    positive       0.86      0.82      0.84       152\n",
      "\n",
      "    accuracy                           0.96      2224\n",
      "   macro avg       0.69      0.60      0.63      2224\n",
      "weighted avg       0.95      0.96      0.95      2224\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "key = 'SCREEN'\n",
    "for idx in range(len(true_full_labels)):\n",
    "    y_true.append(true_full_labels[idx][key])\n",
    "    y_pred.append(predictions_on_test[idx][key])\n",
    "\n",
    "report = classification_report(y_true, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Đánh giá hiệu suất nhận diện tình cảm của khía cạnh FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.85      0.88       526\n",
      "     neutral       0.00      0.00      0.00        52\n",
      "        none       0.94      1.00      0.97      1413\n",
      "    positive       0.82      0.80      0.81       233\n",
      "\n",
      "    accuracy                           0.92      2224\n",
      "   macro avg       0.67      0.66      0.66      2224\n",
      "weighted avg       0.90      0.92      0.91      2224\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "key = 'FEATURES'\n",
    "for idx in range(len(true_full_labels)):\n",
    "    y_true.append(true_full_labels[idx][key])\n",
    "    y_pred.append(predictions_on_test[idx][key])\n",
    "\n",
    "report = classification_report(y_true, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Đánh giá hiệu suất nhận diện tình cảm của khía cạnh PERFORMANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00       454\n",
      "     neutral       0.00      0.00      0.00       116\n",
      "        none       0.47      1.00      0.64      1052\n",
      "    positive       0.00      0.00      0.00       602\n",
      "\n",
      "    accuracy                           0.47      2224\n",
      "   macro avg       0.12      0.25      0.16      2224\n",
      "weighted avg       0.22      0.47      0.30      2224\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "key = 'PERFORMANCE'\n",
    "for idx in range(len(true_full_labels)):\n",
    "    y_true.append(true_full_labels[idx][key])\n",
    "    y_pred.append(predictions_on_test[idx][key])\n",
    "\n",
    "report = classification_report(y_true, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Đánh giá hiệu suất nhận diện tình cảm của khía cạnh GENERAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.94      0.72      0.82       361\n",
      "     neutral       0.00      0.00      0.00        83\n",
      "        none       0.78      1.00      0.88       658\n",
      "    positive       0.95      0.93      0.94      1122\n",
      "\n",
      "    accuracy                           0.88      2224\n",
      "   macro avg       0.67      0.66      0.66      2224\n",
      "weighted avg       0.86      0.88      0.87      2224\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "key = 'GENERAL'\n",
    "for idx in range(len(true_full_labels)):\n",
    "    y_true.append(true_full_labels[idx][key])\n",
    "    y_pred.append(predictions_on_test[idx][key])\n",
    "\n",
    "report = classification_report(y_true, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Đánh giá hiệu suất nhận diện tình cảm của khía cạnh CAMERA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.82      0.82       197\n",
      "     neutral       0.50      0.01      0.03        71\n",
      "        none       0.98      1.00      0.99      1591\n",
      "    positive       0.85      0.95      0.90       365\n",
      "\n",
      "    accuracy                           0.94      2224\n",
      "   macro avg       0.79      0.69      0.68      2224\n",
      "weighted avg       0.93      0.94      0.93      2224\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "key = 'CAMERA'\n",
    "for idx in range(len(true_full_labels)):\n",
    "    y_true.append(true_full_labels[idx][key])\n",
    "    y_pred.append(predictions_on_test[idx][key])\n",
    "\n",
    "report = classification_report(y_true, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Đánh giá hiệu suất nhận diện tình cảm của khía cạnh BATTERY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.89      0.87       397\n",
      "     neutral       0.00      0.00      0.00        92\n",
      "        none       0.99      1.00      0.99      1157\n",
      "    positive       0.85      0.95      0.90       578\n",
      "\n",
      "    accuracy                           0.92      2224\n",
      "   macro avg       0.67      0.71      0.69      2224\n",
      "weighted avg       0.89      0.92      0.91      2224\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "key = 'BATTERY'\n",
    "for idx in range(len(true_full_labels)):\n",
    "    y_true.append(true_full_labels[idx][key])\n",
    "    y_pred.append(predictions_on_test[idx][key])\n",
    "\n",
    "report = classification_report(y_true, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Đánh giá hiệu suất nhận diện tình cảm của khía cạnh STORAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00         6\n",
      "     neutral       0.00      0.00      0.00         3\n",
      "        none       0.99      1.00      0.99      2197\n",
      "    positive       0.00      0.00      0.00        18\n",
      "\n",
      "    accuracy                           0.99      2224\n",
      "   macro avg       0.25      0.25      0.25      2224\n",
      "weighted avg       0.98      0.99      0.98      2224\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "key = 'STORAGE'\n",
    "for idx in range(len(true_full_labels)):\n",
    "    y_true.append(true_full_labels[idx][key])\n",
    "    y_pred.append(predictions_on_test[idx][key])\n",
    "\n",
    "report = classification_report(y_true, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Đánh giá hiệu suất nhận diện tình cảm của khía cạnh DESIGN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.24      0.38       105\n",
      "     neutral       0.00      0.00      0.00        28\n",
      "        none       0.95      1.00      0.97      1794\n",
      "    positive       0.87      0.89      0.88       297\n",
      "\n",
      "    accuracy                           0.94      2224\n",
      "   macro avg       0.68      0.53      0.56      2224\n",
      "weighted avg       0.92      0.94      0.92      2224\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "key = 'DESIGN'\n",
    "for idx in range(len(true_full_labels)):\n",
    "    y_true.append(true_full_labels[idx][key])\n",
    "    y_pred.append(predictions_on_test[idx][key])\n",
    "\n",
    "report = classification_report(y_true, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Đánh giá hiệu suất nhận diện tình cảm của khía cạnh PRICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.50      0.60        90\n",
      "     neutral       0.88      0.75      0.81       361\n",
      "        none       0.96      1.00      0.98      1591\n",
      "    positive       0.75      0.81      0.78       182\n",
      "\n",
      "    accuracy                           0.92      2224\n",
      "   macro avg       0.84      0.76      0.79      2224\n",
      "weighted avg       0.92      0.92      0.92      2224\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "key = 'PRICE'\n",
    "for idx in range(len(true_full_labels)):\n",
    "    y_true.append(true_full_labels[idx][key])\n",
    "    y_pred.append(predictions_on_test[idx][key])\n",
    "\n",
    "report = classification_report(y_true, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Đánh giá hiệu suất nhận diện tình cảm của khía cạnh SER&ACC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.48      0.63       201\n",
      "     neutral       0.00      0.00      0.00        27\n",
      "        none       0.91      1.00      0.95      1560\n",
      "    positive       0.94      0.87      0.90       436\n",
      "\n",
      "    accuracy                           0.92      2224\n",
      "   macro avg       0.69      0.59      0.62      2224\n",
      "weighted avg       0.90      0.92      0.90      2224\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "key = 'SER&ACC'\n",
    "for idx in range(len(true_full_labels)):\n",
    "    y_true.append(true_full_labels[idx][key])\n",
    "    y_pred.append(predictions_on_test[idx][key])\n",
    "\n",
    "report = classification_report(y_true, y_pred)\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
