{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Author: Ngo Van Uc\n",
    "Date: 23/08/2024\n",
    "Gmail: ngovanuc.1508@gmail.com\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 00. Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '/content/drive/MyDrive/RESEARCHING/ASPECT_BASE_SENTIMENT_ANALYSIS/data/Train.csv'\n",
    "test_path = '/content/drive/MyDrive/RESEARCHING/ASPECT_BASE_SENTIMENT_ANALYSIS/data/Test.csv'\n",
    "dev_path = '/content/drive/MyDrive/RESEARCHING/ASPECT_BASE_SENTIMENT_ANALYSIS/data/Dev.csv'\n",
    "\n",
    "train_data = pd.read_csv(train_path)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_to_id = {\"SCREEN\": 0, \"CAMERA\": 1, \"FEATURES\": 2, \"BATTERY\": 3, \"PERFORMANCE\": 4, \"STORAGE\": 5, \"DESIGN\": 6, \"PRICE\": 7, \"GENERAL\": 8, \"SER&ACC\": 9}\n",
    "label_to_id = {\"Positive\": 0, \"Negative\": 1, \"Neutral\": 2, \"None\": 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_dict(sentences_label):\n",
    "    label_dict = {}\n",
    "    labels = sentences_label.split(';')[:-1]\n",
    "    if len(labels) == 1 and labels[-1][1:-1] == \"OTHERS\":\n",
    "        return None\n",
    "    else:\n",
    "        if labels[-1][1:-1] == \"OTHERS\":\n",
    "            for l in labels[:-1]:\n",
    "                class_name, sentiment = l[1:-1].split('#')\n",
    "                label_dict[class_name] = sentiment\n",
    "            return label_dict\n",
    "        else:\n",
    "            for l in labels:\n",
    "                class_name, sentiment = l[1:-1].split('#')\n",
    "                label_dict[class_name] = sentiment\n",
    "            return label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = train_data['comment'].tolist()\n",
    "label_comment = train_data['label'].tolist()\n",
    "others_label = [[0, 0, 0, 1]] * 10\n",
    "\n",
    "train_set = []\n",
    "\n",
    "for sentence, sentence_label in zip(sentences, label_comment):\n",
    "    label_dict = get_label_dict(sentence_label)\n",
    "    labels_ = []\n",
    "    if label_dict:\n",
    "        for aspect in list(class_to_id.keys()):\n",
    "            train_label = [0] * 4\n",
    "            if aspect in label_dict.keys():\n",
    "                label_id = label_to_id[label_dict[aspect]]\n",
    "                train_label[label_id] =  1\n",
    "                labels_.append(train_label)\n",
    "            else:\n",
    "                labels_.append([0, 0, 0, 1])\n",
    "        train_set.append(\n",
    "            {'sentence': sentence, 'labels': labels_}\n",
    "        )\n",
    "    else:\n",
    "        train_set.append(\n",
    "            {'sentence': sentence, 'labels': others_label}\n",
    "        )\n",
    "print(train_set[0]['sentence'])\n",
    "print(train_set[0]['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(train_set):\n",
    "    sentence, label = data.items()\n",
    "    input = np.array(label[-1])\n",
    "    clear_output()\n",
    "    if input.shape == (10, 4):\n",
    "        print(\"pass\")\n",
    "    else:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "\n",
    "class AspectSentimentDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sentence = self.data[index]['sentence']\n",
    "        labels = self.data[index]['labels']\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            sentence,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'sentence_text': sentence,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(labels, dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('google-bert/bert-base-multilingual-cased')\n",
    "dataset = AspectSentimentDataset(train_set, tokenizer, max_len=128)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "\n",
    "\n",
    "class AspectSentimentModel(nn.Module):\n",
    "    def __init__(self, n_classes, n_labels_per_class):\n",
    "        super(AspectSentimentModel, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('google-bert/bert-base-multilingual-cased')\n",
    "        self.lstm = nn.LSTM(768, 256, batch_first=True, bidirectional=True)\n",
    "        self.classifier = nn.Linear(256*2, n_classes * n_labels_per_class)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        bert_outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        lstm_outputs, _ = self.lstm(bert_outputs.last_hidden_state)\n",
    "        lstm_outputs = lstm_outputs[:, -1, :]\n",
    "        output = self.classifier(lstm_outputs)\n",
    "        return output.view(-1, 10, 4)\n",
    "\n",
    "model = AspectSentimentModel(n_classes=10, n_labels_per_class=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04. Training model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 04.1 Traning lần 1 với 20 epochs và lưu model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def train_epoch(model, data_loader, loss_fn, optimizer, device):\n",
    "    model = model.train()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for data in data_loader:\n",
    "        input_ids = data['input_ids'].to(device)\n",
    "        attention_mask = data['attention_mask'].to(device)\n",
    "        labels = data['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return np.mean(losses)\n",
    "\n",
    "\n",
    "EPOCHS = 20\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "training = True\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "if training == True:\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss = train_epoch(model, dataloader, loss_fn, optimizer, device)\n",
    "        print(f'Epoch {epoch + 1}/{EPOCHS}, Train loss: {train_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_save_model = '/content/drive/MyDrive/RESEARCHING/ASPECT_BASE_SENTIMENT_ANALYSIS'\n",
    "torch.save(model.state_dict(), path_save_model+'/model_20epochs.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 04.2 Tải lại model sau đó training lần 2 với 20 epochs và lưu model\n",
    "Mục đích của việc làm này:\n",
    "- colab có miễn phí GPU nhưng có giới hạn, việc huấn luyện model trong thời gian dài sẽ bị ngắt kết nối. chia ra nhiều lần huấn luyện để có thể tận dụng GPU miễn phí mà vẫn đảm bảo quá trình training theo kế hoạch\n",
    "- ngoài ra, nếu training lần nào thì chạy lần đó, không chạy lại các lần training trước"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_save_model = '/content/drive/MyDrive/RESEARCHING/ASPECT_BASE_SENTIMENT_ANALYSIS'\n",
    "model = AspectSentimentModel(n_classes=10, n_labels_per_class=4)\n",
    "model.load_state_dict(torch.load(path_save_model+'/model_20epochs.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "def train_epoch(model, data_loader, loss_fn, optimizer, device):\n",
    "    model = model.train()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for data in data_loader:\n",
    "        input_ids = data['input_ids'].to(device)\n",
    "        attention_mask = data['attention_mask'].to(device)\n",
    "        labels = data['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return np.mean(losses)\n",
    "\n",
    "\n",
    "EPOCHS = 20\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "training = True\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "if training == True:\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss = train_epoch(model, dataloader, loss_fn, optimizer, device)\n",
    "        print(f'Epoch {epoch + 21}/{EPOCHS+20}, Train loss: {train_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_save_model = '/content/drive/MyDrive/RESEARCHING/ASPECT_BASE_SENTIMENT_ANALYSIS'\n",
    "torch.save(model.state_dict(), path_save_model+'/model_40epochs.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 04.3 Tải lại model sau đó training lần 3 với 10 epochs rồi lưu model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_save_model = '/content/drive/MyDrive/RESEARCHING/ASPECT_BASE_SENTIMENT_ANALYSIS'\n",
    "model = AspectSentimentModel(n_classes=10, n_labels_per_class=4)\n",
    "model.load_state_dict(torch.load(path_save_model+'/model_40epochs.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "def train_epoch(model, data_loader, loss_fn, optimizer, device):\n",
    "    model = model.train()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for data in data_loader:\n",
    "        input_ids = data['input_ids'].to(device)\n",
    "        attention_mask = data['attention_mask'].to(device)\n",
    "        labels = data['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return np.mean(losses)\n",
    "\n",
    "\n",
    "EPOCHS = 10\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "training = True\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "if training == True:\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss = train_epoch(model, dataloader, loss_fn, optimizer, device)\n",
    "        print(f'Epoch {epoch + 41}/{EPOCHS+40}, Train loss: {train_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_save_model = '/content/drive/MyDrive/RESEARCHING/ASPECT_BASE_SENTIMENT_ANALYSIS'\n",
    "torch.save(model.state_dict(), path_save_model+'/model_50epochs.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05. Xây dựng bộ Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(model, sentence, tokenizer, max_len, device):\n",
    "    model = model.eval()\n",
    "\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        sentence,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_len,\n",
    "        return_token_type_ids=False,\n",
    "        pad_to_max_length=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def interpret_predictions(predictions):\n",
    "    aspect_classes = ['SCREEN', 'CAMERA', 'FEATURES', 'BATTERY', 'PERFORMANCE', 'STORAGE', 'DESIGN', 'PRICE', 'GENERAL', 'SER&ACC']\n",
    "    sentiment_labels = ['Positive', 'Negative', 'Neutral', 'None']\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for i, aspect in enumerate(aspect_classes):\n",
    "        aspect_sentiments = predictions[0, i].cpu().numpy()\n",
    "        sentiment_index = aspect_sentiments.argmax()\n",
    "        results[aspect] = sentiment_labels[sentiment_index]\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06. Thực hiện predict trên dữ liệu kiểm thử"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(test_path)\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 256\n",
    "idx = 20\n",
    "sentence_test = test_data.loc[idx, 'comment']\n",
    "true_label = test_data.loc[idx, 'label']\n",
    "print(sentence_test)\n",
    "print(true_label)\n",
    "\n",
    "predictions = predict_sentiment(model, sentence_test, tokenizer, max_len, device)\n",
    "results = interpret_predictions(predictions)\n",
    "\n",
    "print('Aspect Sentiment Prediction:')\n",
    "for aspect, sentiment in results.items():\n",
    "    print(f'{aspect}: {sentiment}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_on_test = []\n",
    "\n",
    "for i in range(len(test_data)):\n",
    "    print(i)\n",
    "    sentence_on_test = test_data['comment'][i]\n",
    "    prediction_on_test = predict_sentiment(model, sentence_on_test, tokenizer, max_len, device)\n",
    "    results = interpret_predictions(prediction_on_test)\n",
    "    predictions_on_test.append(results)\n",
    "    clear_output()\n",
    "\n",
    "print('All done!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = []\n",
    "\n",
    "for i in range(len(test_data)):\n",
    "    outcome = {}\n",
    "    label = test_data['label'][i]\n",
    "    label = label.replace('{', '')\n",
    "    label = label.replace('}', '')\n",
    "    labels = label.split(';')\n",
    "    for l in labels:\n",
    "        try:\n",
    "            text = l.split('#')\n",
    "            outcome[text[0]] = text[1]\n",
    "        except:\n",
    "            pass\n",
    "    true_labels.append(outcome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07. Thực hiện chuẩn hóa nhãn predict và nhãn true về cùng định dạng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_label(true_label, prediction):\n",
    "    '''true_label: dict\n",
    "     prediction: dict '''\n",
    "    for key, value in prediction.items():\n",
    "        if key not in true_label:\n",
    "            true_label[key] = 'None'\n",
    "    return true_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mục tiêu là đưa danh sách các nhãn thực tế về dạng đánh giá 10 class như trong dự đoán\n",
    "true_full_labels = []\n",
    "for idx in range(len(predictions_on_test)):\n",
    "    true_label = true_labels[idx]\n",
    "    prediction = predictions_on_test[idx]\n",
    "    true_full_labels.append(full_label(true_label, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data thỉnh thoảng bị sai -..-\n",
    "# bằng chứng là SER&ACC trong câu này phải là Positive nhưng nhãn là None\n",
    "# hoặc GENERAL nên là Positive thì nhãn lại là None\n",
    "\n",
    "print(predictions_on_test[6])\n",
    "print(true_labels[6])\n",
    "print(test_data.loc[6, 'comment'])\n",
    "for key, value in true_labels[6].items():\n",
    "    print('true: ',key, value)\n",
    "    print('predicted: ', key, predictions_on_test[6][key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 08. Đánh giá hiệu suất tổng thể của mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_ = []\n",
    "predicted_ = []\n",
    "\n",
    "for idx in range(len(predictions_on_test)):\n",
    "    for key, value in predictions_on_test[idx].items():\n",
    "        if true_full_labels[idx][key] == 'None' and value == 'None':\n",
    "            true_.append(0)\n",
    "            predicted_.append(0)\n",
    "        elif true_full_labels[idx][key] != 'None' and true_full_labels[idx][key] == value:\n",
    "            true_.append(1)\n",
    "            predicted_.append(1)\n",
    "        elif true_full_labels[idx][key] == 'None' and value != 'None':\n",
    "            true_.append(0)\n",
    "            predicted_.append(1)\n",
    "        elif true_full_labels[idx][key] != 'None' and value == 'None':\n",
    "            true_.append(1)\n",
    "            predicted_.append(0)\n",
    "\n",
    "accuracy = accuracy_score(true_, predicted_)\n",
    "precision = precision_score(true_, predicted_)\n",
    "recall = recall_score(true_, predicted_)\n",
    "f1 = f1_score(true_, predicted_)\n",
    "\n",
    "print(f'Accuracy = {accuracy}')\n",
    "print(f'Precision = {precision}')\n",
    "print(f'Recall = {recall}')\n",
    "print(f'f1 = {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 09. Đánh giá hiệu suất nhận diên khía cạnh của mô hình"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 09.1 Accuracy, Precision, Recall, F1-score trên toàn bộ khía cạnh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_ = []\n",
    "predicted_ = []\n",
    "\n",
    "for idx in range(len(predictions_on_test)):\n",
    "    for key, value in predictions_on_test[idx].items():\n",
    "        if true_full_labels[idx] == 'None' and value == 'None':\n",
    "            true_.append(0)\n",
    "            predicted_.append(0)\n",
    "        elif true_full_labels[idx][key] != 'None' and value != 'None':\n",
    "            true_.append(1)\n",
    "            predicted_.append(1)\n",
    "        elif true_full_labels[idx][key] == 'None' and value != 'None':\n",
    "            true_.append(0)\n",
    "            predicted_.append(1)\n",
    "        elif true_full_labels[idx][key] != 'None' and value == 'None':\n",
    "            true_.append(1)\n",
    "            predicted_.append(0)\n",
    "\n",
    "print(f'Accuracy = {accuracy_score(true_, predicted_)}')\n",
    "print(f'Precision = {precision_score(true_, predicted_)}')\n",
    "print(f'Recall = {recall_score(true_, predicted_)}')\n",
    "print(f'F1-score = {f1_score(true_, predicted_)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 09.2 Đánh giá hiệu suất nhận diện khía cạnh SCREEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_ = []\n",
    "predicted_ = []\n",
    "\n",
    "for idx in range(len(true_full_labels)):\n",
    "    key = 'SCREEN'\n",
    "    if true_full_labels[idx][key] == 'None' and predictions_on_test[idx][key] == 'None':\n",
    "        true_.append(0)\n",
    "        predicted_.append(0)\n",
    "    elif true_full_labels[idx][key] != 'None' and predictions_on_test[idx][key] != 'None':\n",
    "        true_.append(1)\n",
    "        predicted_.append(1)\n",
    "    elif true_full_labels[idx][key] == 'None' and predictions_on_test[idx][key] != 'None':\n",
    "        true_.append(0)\n",
    "        predicted_.append(1)\n",
    "    elif true_full_labels[idx][key] != 'None' and predictions_on_test[idx][key] == 'None':\n",
    "        true_.append(1)\n",
    "        predicted_.append(0)\n",
    "\n",
    "accuracy = accuracy_score(true_, predicted_)\n",
    "precision = precision_score(true_, predicted_)\n",
    "recall = recall_score(true_, predicted_)\n",
    "f1 = f1_score(true_, predicted_)\n",
    "\n",
    "print(f'Accuracy = {accuracy}')\n",
    "print(f'Precision = {precision}')\n",
    "print(f'Recall = {recall}')\n",
    "print(f'f1 = {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 09.3 Đánh giá hiệu suất nhận diện khía cạnh FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_ = []\n",
    "predicted_ = []\n",
    "\n",
    "for idx in range(len(true_full_labels)):\n",
    "    key = 'FEATURES'\n",
    "    if true_full_labels[idx][key] == 'None' and predictions_on_test[idx][key] == 'None':\n",
    "        true_.append(0)\n",
    "        predicted_.append(0)\n",
    "    elif true_full_labels[idx][key] != 'None' and predictions_on_test[idx][key] != 'None':\n",
    "        true_.append(1)\n",
    "        predicted_.append(1)\n",
    "    elif true_full_labels[idx][key] == 'None' and predictions_on_test[idx][key] != 'None':\n",
    "        true_.append(0)\n",
    "        predicted_.append(1)\n",
    "    elif true_full_labels[idx][key] != 'None' and predictions_on_test[idx][key] == 'None':\n",
    "        true_.append(1)\n",
    "        predicted_.append(0)\n",
    "\n",
    "accuracy = accuracy_score(true_, predicted_)\n",
    "precision = precision_score(true_, predicted_)\n",
    "recall = recall_score(true_, predicted_)\n",
    "f1 = f1_score(true_, predicted_)\n",
    "\n",
    "print(f'Accuracy = {accuracy}')\n",
    "print(f'Precision = {precision}')\n",
    "print(f'Recall = {recall}')\n",
    "print(f'f1 = {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 09.4 Đánh giá hiệu suất nhận diện khía cạnh PERFORMANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_ = []\n",
    "predicted_ = []\n",
    "\n",
    "for idx in range(len(true_full_labels)):\n",
    "    key = 'PERFORMANCE'\n",
    "    if true_full_labels[idx][key] == 'None' and predictions_on_test[idx][key] == 'None':\n",
    "        true_.append(0)\n",
    "        predicted_.append(0)\n",
    "    elif true_full_labels[idx][key] != 'None' and predictions_on_test[idx][key] != 'None':\n",
    "        true_.append(1)\n",
    "        predicted_.append(1)\n",
    "    elif true_full_labels[idx][key] == 'None' and predictions_on_test[idx][key] != 'None':\n",
    "        true_.append(0)\n",
    "        predicted_.append(1)\n",
    "    elif true_full_labels[idx][key] != 'None' and predictions_on_test[idx][key] == 'None':\n",
    "        true_.append(1)\n",
    "        predicted_.append(0)\n",
    "\n",
    "accuracy = accuracy_score(true_, predicted_)\n",
    "precision = precision_score(true_, predicted_)\n",
    "recall = recall_score(true_, predicted_)\n",
    "f1 = f1_score(true_, predicted_)\n",
    "\n",
    "print(f'Accuracy = {accuracy}')\n",
    "print(f'Precision = {precision}')\n",
    "print(f'Recall = {recall}')\n",
    "print(f'f1 = {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 09.5 Đánh giá hiệu suất nhận diện khía cạnh GENERAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_ = []\n",
    "predicted_ = []\n",
    "\n",
    "for idx in range(len(true_full_labels)):\n",
    "    key = 'GENERAL'\n",
    "    if true_full_labels[idx][key] == 'None' and predictions_on_test[idx][key] == 'None':\n",
    "        true_.append(0)\n",
    "        predicted_.append(0)\n",
    "    elif true_full_labels[idx][key] != 'None' and predictions_on_test[idx][key] != 'None':\n",
    "        true_.append(1)\n",
    "        predicted_.append(1)\n",
    "    elif true_full_labels[idx][key] == 'None' and predictions_on_test[idx][key] != 'None':\n",
    "        true_.append(0)\n",
    "        predicted_.append(1)\n",
    "    elif true_full_labels[idx][key] != 'None' and predictions_on_test[idx][key] == 'None':\n",
    "        true_.append(1)\n",
    "        predicted_.append(0)\n",
    "\n",
    "accuracy = accuracy_score(true_, predicted_)\n",
    "precision = precision_score(true_, predicted_)\n",
    "recall = recall_score(true_, predicted_)\n",
    "f1 = f1_score(true_, predicted_)\n",
    "\n",
    "print(f'Accuracy = {accuracy}')\n",
    "print(f'Precision = {precision}')\n",
    "print(f'Recall = {recall}')\n",
    "print(f'f1 = {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 09.6 Đánh giá hiệu suất nhận diện khía cạnh GENERAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_ = []\n",
    "predicted_ = []\n",
    "\n",
    "for idx in range(len(true_full_labels)):\n",
    "    key = 'BATTERY'\n",
    "    if true_full_labels[idx][key] == 'None' and predictions_on_test[idx][key] == 'None':\n",
    "        true_.append(0)\n",
    "        predicted_.append(0)\n",
    "    elif true_full_labels[idx][key] != 'None' and predictions_on_test[idx][key] != 'None':\n",
    "        true_.append(1)\n",
    "        predicted_.append(1)\n",
    "    elif true_full_labels[idx][key] == 'None' and predictions_on_test[idx][key] != 'None':\n",
    "        true_.append(0)\n",
    "        predicted_.append(1)\n",
    "    elif true_full_labels[idx][key] != 'None' and predictions_on_test[idx][key] == 'None':\n",
    "        true_.append(1)\n",
    "        predicted_.append(0)\n",
    "\n",
    "accuracy = accuracy_score(true_, predicted_)\n",
    "precision = precision_score(true_, predicted_)\n",
    "recall = recall_score(true_, predicted_)\n",
    "f1 = f1_score(true_, predicted_)\n",
    "\n",
    "print(f'Accuracy = {accuracy}')\n",
    "print(f'Precision = {precision}')\n",
    "print(f'Recall = {recall}')\n",
    "print(f'f1 = {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 09.7 Đánh giá hiệu suất nhận diện khía cạnh STORAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_ = []\n",
    "predicted_ = []\n",
    "\n",
    "for idx in range(len(true_full_labels)):\n",
    "    key = 'STORAGE'\n",
    "    if true_full_labels[idx][key] == 'None' and predictions_on_test[idx][key] == 'None':\n",
    "        true_.append(0)\n",
    "        predicted_.append(0)\n",
    "    elif true_full_labels[idx][key] != 'None' and predictions_on_test[idx][key] != 'None':\n",
    "        true_.append(1)\n",
    "        predicted_.append(1)\n",
    "    elif true_full_labels[idx][key] == 'None' and predictions_on_test[idx][key] != 'None':\n",
    "        true_.append(0)\n",
    "        predicted_.append(1)\n",
    "    elif true_full_labels[idx][key] != 'None' and predictions_on_test[idx][key] == 'None':\n",
    "        true_.append(1)\n",
    "        predicted_.append(0)\n",
    "\n",
    "accuracy = accuracy_score(true_, predicted_)\n",
    "precision = precision_score(true_, predicted_)\n",
    "recall = recall_score(true_, predicted_)\n",
    "f1 = f1_score(true_, predicted_)\n",
    "\n",
    "print(f'Accuracy = {accuracy}')\n",
    "print(f'Precision = {precision}')\n",
    "print(f'Recall = {recall}')\n",
    "print(f'f1 = {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 09.8 Đánh giá hiệu suất nhận diện khía cạnh DESIGN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_ = []\n",
    "predicted_ = []\n",
    "\n",
    "for idx in range(len(true_full_labels)):\n",
    "    key = 'DESIGN'\n",
    "    if true_full_labels[idx][key] == 'None' and predictions_on_test[idx][key] == 'None':\n",
    "        true_.append(0)\n",
    "        predicted_.append(0)\n",
    "    elif true_full_labels[idx][key] != 'None' and predictions_on_test[idx][key] != 'None':\n",
    "        true_.append(1)\n",
    "        predicted_.append(1)\n",
    "    elif true_full_labels[idx][key] == 'None' and predictions_on_test[idx][key] != 'None':\n",
    "        true_.append(0)\n",
    "        predicted_.append(1)\n",
    "    elif true_full_labels[idx][key] != 'None' and predictions_on_test[idx][key] == 'None':\n",
    "        true_.append(1)\n",
    "        predicted_.append(0)\n",
    "\n",
    "accuracy = accuracy_score(true_, predicted_)\n",
    "precision = precision_score(true_, predicted_)\n",
    "recall = recall_score(true_, predicted_)\n",
    "f1 = f1_score(true_, predicted_)\n",
    "\n",
    "print(f'Accuracy = {accuracy}')\n",
    "print(f'Precision = {precision}')\n",
    "print(f'Recall = {recall}')\n",
    "print(f'f1 = {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 09.9 Đánh giá hiệu suất nhận diện khía cạnh PRICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_ = []\n",
    "predicted_ = []\n",
    "\n",
    "for idx in range(len(true_full_labels)):\n",
    "    key = 'PRICE'\n",
    "    if true_full_labels[idx][key] == 'None' and predictions_on_test[idx][key] == 'None':\n",
    "        true_.append(0)\n",
    "        predicted_.append(0)\n",
    "    elif true_full_labels[idx][key] != 'None' and predictions_on_test[idx][key] != 'None':\n",
    "        true_.append(1)\n",
    "        predicted_.append(1)\n",
    "    elif true_full_labels[idx][key] == 'None' and predictions_on_test[idx][key] != 'None':\n",
    "        true_.append(0)\n",
    "        predicted_.append(1)\n",
    "    elif true_full_labels[idx][key] != 'None' and predictions_on_test[idx][key] == 'None':\n",
    "        true_.append(1)\n",
    "        predicted_.append(0)\n",
    "\n",
    "accuracy = accuracy_score(true_, predicted_)\n",
    "precision = precision_score(true_, predicted_)\n",
    "recall = recall_score(true_, predicted_)\n",
    "f1 = f1_score(true_, predicted_)\n",
    "\n",
    "print(f'Accuracy = {accuracy}')\n",
    "print(f'Precision = {precision}')\n",
    "print(f'Recall = {recall}')\n",
    "print(f'f1 = {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 09.10 Đánh giá hiệu suất nhận diện khía cạnh SER&ACC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_ = []\n",
    "predicted_ = []\n",
    "\n",
    "for idx in range(len(true_full_labels)):\n",
    "    key = 'SER&ACC'\n",
    "    if true_full_labels[idx][key] == 'None' and predictions_on_test[idx][key] == 'None':\n",
    "        true_.append(0)\n",
    "        predicted_.append(0)\n",
    "    elif true_full_labels[idx][key] != 'None' and predictions_on_test[idx][key] != 'None':\n",
    "        true_.append(1)\n",
    "        predicted_.append(1)\n",
    "    elif true_full_labels[idx][key] == 'None' and predictions_on_test[idx][key] != 'None':\n",
    "        true_.append(0)\n",
    "        predicted_.append(1)\n",
    "    elif true_full_labels[idx][key] != 'None' and predictions_on_test[idx][key] == 'None':\n",
    "        true_.append(1)\n",
    "        predicted_.append(0)\n",
    "\n",
    "accuracy = accuracy_score(true_, predicted_)\n",
    "precision = precision_score(true_, predicted_)\n",
    "recall = recall_score(true_, predicted_)\n",
    "f1 = f1_score(true_, predicted_)\n",
    "\n",
    "print(f'Accuracy = {accuracy}')\n",
    "print(f'Precision = {precision}')\n",
    "print(f'Recall = {recall}')\n",
    "print(f'f1 = {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 09.11 Đánh giá hiệu suất nhận diện khía cạnh CAMERA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_ = []\n",
    "predicted_ = []\n",
    "\n",
    "for idx in range(len(true_full_labels)):\n",
    "    key = 'CAMERA'\n",
    "    if true_full_labels[idx][key] == 'None' and predictions_on_test[idx][key] == 'None':\n",
    "        true_.append(0)\n",
    "        predicted_.append(0)\n",
    "    elif true_full_labels[idx][key] != 'None' and predictions_on_test[idx][key] != 'None':\n",
    "        true_.append(1)\n",
    "        predicted_.append(1)\n",
    "    elif true_full_labels[idx][key] == 'None' and predictions_on_test[idx][key] != 'None':\n",
    "        true_.append(0)\n",
    "        predicted_.append(1)\n",
    "    elif true_full_labels[idx][key] != 'None' and predictions_on_test[idx][key] == 'None':\n",
    "        true_.append(1)\n",
    "        predicted_.append(0)\n",
    "\n",
    "accuracy = accuracy_score(true_, predicted_)\n",
    "precision = precision_score(true_, predicted_)\n",
    "recall = recall_score(true_, predicted_)\n",
    "f1 = f1_score(true_, predicted_)\n",
    "\n",
    "print(f'Accuracy = {accuracy}')\n",
    "print(f'Precision = {precision}')\n",
    "print(f'Recall = {recall}')\n",
    "print(f'f1 = {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Đánh giá hiệu suất phân loại tình cảm (đa nhãn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1 Accuracy trên toàn bộ data kiểm thử"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_ = []\n",
    "predicted_ = []\n",
    "accuracy = []\n",
    "\n",
    "for idx in range(len(true_full_labels)):\n",
    "    for key, value in true_full_labels[idx].items():\n",
    "        if value == 'None':\n",
    "            continue\n",
    "        elif predictions_on_test[idx][key] != value:\n",
    "            accuracy.append(0)\n",
    "        elif predictions_on_test[idx][key] == value:\n",
    "            accuracy.append(1)\n",
    "\n",
    "print(f'Accuracy = {sum(accuracy)/len(accuracy)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2 Precision, Recall, F1-score trên toàn bộ data kiểm thử"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for idx in range(len(true_labels)):\n",
    "    for key, value in true_labels[idx].items():\n",
    "        y_true.append(key + \"_\" + value)\n",
    "        y_pred.append(key + \"_\" + predictions_on_test[idx][key])\n",
    "\n",
    "\n",
    "print(f'Classification report:')\n",
    "print(classification_report(y_true, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_micro = precision_score(y_true, y_pred, average='micro')\n",
    "recall_micro = recall_score(y_true, y_pred, average='micro')\n",
    "f1_micro = f1_score(y_true, y_pred, average='micro')\n",
    "\n",
    "precision_macro = precision_score(y_true, y_pred, average='macro')\n",
    "recall_macro = recall_score(y_true, y_pred, average='macro')\n",
    "f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "precision_weighted = precision_score(y_true, y_pred, average='weighted')\n",
    "recall_weighted = recall_score(y_true, y_pred, average='weighted')\n",
    "f1_weighted = f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "print(\"\\nOverall Precision, Recall, F1-Score:\")\n",
    "print(f\"Micro Precision: {precision_micro:.4f}\")\n",
    "print(f\"Micro Recall: {recall_micro:.4f}\")\n",
    "print(f\"Micro F1-Score: {f1_micro:.4f}\")\n",
    "print(f\"Macro Precision: {precision_macro:.4f}\")\n",
    "print(f\"Macro Recall: {recall_macro:.4f}\")\n",
    "print(f\"Macro F1-Score: {f1_macro:.4f}\")\n",
    "print(f\"Weighted Precision: {precision_weighted:.4f}\")\n",
    "print(f\"Weighted Recall: {recall_weighted:.4f}\")\n",
    "print(f\"Weighted F1-Score: {f1_weighted:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3 Đánh giá hiệu suất phân loại tình cảm trên khía cạnh SCREEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "key = 'SCREEN'\n",
    "for idx in range(len(true_full_labels)):\n",
    "    y_true.append(true_full_labels[idx][key])\n",
    "    y_pred.append(predictions_on_test[idx][key])\n",
    "\n",
    "report = classification_report(y_true, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.4 Đánh giá hiệu suất phân loại tình cảm trên khía cạnh FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "key = 'FEATURES'\n",
    "for idx in range(len(true_full_labels)):\n",
    "    y_true.append(true_full_labels[idx][key])\n",
    "    y_pred.append(predictions_on_test[idx][key])\n",
    "\n",
    "report = classification_report(y_true, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.5 Đánh giá hiệu suất phân loại tình cảm trên khía cạnh PERFORMANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "key = 'PERFORMANCE'\n",
    "for idx in range(len(true_full_labels)):\n",
    "    y_true.append(true_full_labels[idx][key])\n",
    "    y_pred.append(predictions_on_test[idx][key])\n",
    "\n",
    "report = classification_report(y_true, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.6 Đánh giá hiệu suất phân loại tình cảm trên khía cạnh GENERAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "key = 'GENERAL'\n",
    "for idx in range(len(true_full_labels)):\n",
    "    y_true.append(true_full_labels[idx][key])\n",
    "    y_pred.append(predictions_on_test[idx][key])\n",
    "\n",
    "report = classification_report(y_true, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.7 Đánh giá hiệu suất phân loại tình cảm trên khía cạnh CAMERA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "key = 'CAMERA'\n",
    "for idx in range(len(true_full_labels)):\n",
    "    y_true.append(true_full_labels[idx][key])\n",
    "    y_pred.append(predictions_on_test[idx][key])\n",
    "\n",
    "report = classification_report(y_true, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.8 Đánh giá hiệu suất phân loại tình cảm trên khía cạnh BATTERY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "key = 'BATTERY'\n",
    "for idx in range(len(true_full_labels)):\n",
    "    y_true.append(true_full_labels[idx][key])\n",
    "    y_pred.append(predictions_on_test[idx][key])\n",
    "\n",
    "report = classification_report(y_true, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.9 Đánh giá hiệu suất phân loại tình cảm trên khía cạnh STORAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "key = 'STORAGE'\n",
    "for idx in range(len(true_full_labels)):\n",
    "    y_true.append(true_full_labels[idx][key])\n",
    "    y_pred.append(predictions_on_test[idx][key])\n",
    "\n",
    "report = classification_report(y_true, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.10 Đánh giá hiệu suất phân loại tình cảm trên khía cạnh DESIGN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "key = 'DESIGN'\n",
    "for idx in range(len(true_full_labels)):\n",
    "    y_true.append(true_full_labels[idx][key])\n",
    "    y_pred.append(predictions_on_test[idx][key])\n",
    "\n",
    "report = classification_report(y_true, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.11 Đánh giá hiệu suất phân loại tình cảm trên khía cạnh PRICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "key = 'PRICE'\n",
    "for idx in range(len(true_full_labels)):\n",
    "    y_true.append(true_full_labels[idx][key])\n",
    "    y_pred.append(predictions_on_test[idx][key])\n",
    "\n",
    "report = classification_report(y_true, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.12 Đánh giá hiệu suất phân loại tình cảm trên khía cạnh SER&ACC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "key = 'SER&ACC'\n",
    "for idx in range(len(true_full_labels)):\n",
    "    y_true.append(true_full_labels[idx][key])\n",
    "    y_pred.append(predictions_on_test[idx][key])\n",
    "\n",
    "report = classification_report(y_true, y_pred)\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
